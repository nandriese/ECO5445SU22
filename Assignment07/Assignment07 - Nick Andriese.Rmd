---
title: "Assignment 7"
author: "Nick Andriese"
date: '2022-07-24'
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: console
---
##Some of this is from my assignment 6 code
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}

#install.packages("car","gvlma", "MASS", "leaps")
library(car)
library(gvlma)
library(MASS)
library(leaps)
library(readr)
prprice <- read_csv("prop_prices_reduced.csv")


```

1. This time, I want you to separate your data into testing and training. For this exercise, randomly extract 100 for testing different models, and save the other 900 for training your models

```{r}

#Assignment07 

#1
library(gamlr)
library(pROC)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(e1071)
library(psych)

set.seed(1234)
train <- sample(nrow(prprice[-7,-8]), 0.9*nrow(prprice[-7,-8]))

p.train <- prprice[train,-8]

p.train <- data.frame(p.train)

p.validate <- prprice[-train,-8]

p.validate <- data.frame(p.validate)

```

2. Run your final model you had in the previous assignment to the training data

```{r}
#2

train.model <- lm((sale_def^(.22)) ~ bed + bath + area_heated + area + dist_cbd, data = p.train)
summary(train.model)
plot(train.model)
summary(gvlma(train.model))
```

3. With the same model in part 2, run the standard LASSO regression model on the training data.


```{r}
#Lasso

sessionInfo()
?gamlr
data(p.train)

x <- cbind(p.train$bed, p.train$bath, p.train$area_heated, p.train$area, p.train$dist_cbd, p.train$dist_lakes)
reg <- gamlr(x, p.train$sale_def^.22, verb = TRUE, standardize = TRUE)

exp(coef(reg)[1])


```

4. Now using the same model in part 2, run a 10-fold cross-validated LASSO on the training data

```{r}
#Lasso 10 fold cross validation

cv.reg <- cv.gamlr(x, p.train$sale_def^.22, nfold = 10, verb = TRUE, standardize = TRUE)
par(mfrow = c(1,1))
plot(cv.reg)
plot(cv.reg$gamlr)### this plot just seems so different from the example plot so I'm not sure what might be up.

```



```{r}
#calculate RMSE for each lambda's selection method and the model in part w: AIC, BIC, AICc, cv.min, cv.1se

log(reg$lambda[which.min(AICc(reg))])
log(reg$lambda[which.min(AIC(reg))])
log(reg$lambda[which.min(BIC(reg))])
(cv.reg$lambda.min)
(cv.reg$lambda.1se)


# AICc, AIC, and BIC are all the same, 1se and min are different
```

Plot Cross-validation

```{r}
#plots

plot(cv.reg)

ll <- log(reg$lambda)
plot(reg, col="grey")
abline(v=ll[which.min(AICc(reg))], col="black", lty=2)
abline(v=ll[which.min(AIC(reg))], col="orange", lty=2)
abline(v=ll[which.min(BIC(reg))], col="green", lty=2)
abline(v=log(cv.reg$lambda.min), col="blue", lty=2)
abline(v=log(cv.reg$lambda.1se), col="purple", lty=2)
legend("topright", bty="n", lwd=1, 
       
    col=c("black","orange","green","blue","purple"),
    legend=c("AICc","AIC","BIC","CV.min","CV.1se"))
```


5. Lastly, using the testing data, I want you to calculate the RMSE for each of the lambda's selection methods discussed (AIC, BIC, AICc, cv.min, cv.1se) and the the model in part 2. Which method performed the best in prediction the home price?

```{r}
#RMSE
#install.packages("Metrics")
library(Metrics)

p.validate1 <- data.frame(p.validate[,-1])

pred <- predict(train.model, p.validate1)^(1/.22)
actual <- p.validate$sale_def
modelRMSE <- rmse(actual, pred)

#x1<- data.frame(p.validate$bed, p.validate$bath, p.validate$area_heated, p.validate$area, p.validate$dist_cbd, as.factor(p.validate$pool))

predAICc <- as.matrix(predict(reg, newdata = p.validate1, select =which.min(AICc(reg))))
AICc_RMSE <- rmse(actual, predAICc)

predAIC <- as.matrix(predict(reg, newdata = p.validate1, select =which.min(AIC(reg))))
AIC_RMSE <- rmse(actual, predAIC)

predBIC <- as.matrix(predict(reg, newdata = p.validate1, select =which.min(BIC(reg))))
BIC_RMSE <- rmse(actual, predBIC)

pred_min <- as.matrix(predict(cv.reg, newdata = p.validate1, select ="min"))
min_RMSE <- rmse(actual, pred_min)

pred_1se <- as.matrix(predict(cv.reg, newdata = p.validate1, select ="1se"))
one.se_RMSE <- rmse(actual, pred_1se)

```

Model Comparison

```{r}
modelRMSE
AICc_RMSE
AIC_RMSE
BIC_RMSE
min_RMSE
one.se_RMSE

```

After comparing all of the models, the original model had the lowest RMSE 75284, while all of the rest of the models had virtually the same RMSE (236815). Thus, the original model performed the best in terms  home price predicitons.


End
